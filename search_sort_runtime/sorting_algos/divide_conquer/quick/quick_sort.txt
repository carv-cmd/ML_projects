What is the Quick Sort Algorithm?

For starters it is widely deemed as the 'best' sorting algorithm
Which is just another way of saying the 'one of the most efficient sorting algorithms available'
> If another sorting algorithm is deemed better, it's usually an application specific instance
-> These are usually just combinations of quick sort with some special sauce
-> Handling exceptions where Quick Sort alone otherwise wouldn't exhibit the optimal runtime complexity

################################################################
################################################################
The idea behind Quick Sort is that we have so called "Pivot Elements"

Given listQ = [3, 1, 6, 7, 2, 8, 4, (5)]

Pick a so called 'pivot element', usually the first or last element in a list
Must be a specific one though, 'pivot_always==listQ[1]' or 'pivot_always==listQ[-1]'

Once this element is picked, we arrange all the other elements,
Such that the larger elements are on the right side, & smaller elements on the left side

> Initial == [3, 1, 6, 7, 2, 8, 4, (5)]

-> Shift_1 = [3, 1, 6, 7, 2, (4, 5)<, <(8)]
-> Shift_2 = [3, 1, 6, (2, 4, 5)<, <(7, 8)]
-> Shift_3 = [3, 1, (2, 4, 5)<, <(6, 7, 8)]

--> Result = [(3, 1, 2, 4) < (5) < (6, 7, 8)]

Notice the order of the smaller/larger elements remains the same
> 3, 1, 2, 4 is the same order of these elements if everything but those were removed from Initial
> 6, 7, 8 exhibits the same pattern
So the order of the individual elements stays the same
-> All were really changing is the 5's position with respect to touched_lower < 5 < touched_higher
--> This is considered a 'stable' sort as were maintaining the order

Now we have a 5 already positioned in the right place since lowers < 5 < highers
Next we divide, to conquer later, by splitting the list into 2 parts

Result = [[3, 1, 2, 4] < (5) < [6, 7, 8]]
s1 = [3, 1, 2, 4]
s2 = [6, 7, 8]

Then just recursively repeat this process

> s1 = [3, 1, 2, (4)]
> s1 = [[3, 1, 2], (4)]
-> (4) is greater than all the other elements in the list so its already in the correct position

> s2 = [6, 7, (8)]
> s2 = [[6, 7], (8)]
-> (8) is greater than 6 and 7 so again it's positioned correctly

Since neither of these have elements to the right we don't split each list into 2 lists
Instead we just recursively call pivot on the list elements less than 4, and 8
With these lists being respectively:
-> s1.1 = [3, 1, 2]
-> s2.1 = [6, 7]

With the next pivot these lists become:
> s1.1 = [3, 1, (2)]
> s1.1 = [(1, 2), 3]
> s1.1 = [1, (2), 3]
Since s1.1 has a larger and smaller side this gets split into 2 single element lists

> s2.1 = [6, 7]
> s2.1 = [6, (7)]

Resulting in:
-> s1.1.1 = [1]
-> s1.1.2 = [3]

-> s2.1.1 = [6]

Now we have a sorted list if we look back up through the tree in an index aligned context
This is already conquered, in the sense that we don't have to merge the same way as merge sort
We just have to split them up in the right manner and we end up with a sorted list

Everytime the pivot elements used to arrange the list elements in the right order
** Relative to the pivot element
-> Then we split into two lists, if necessary
-> And with each additional recursion the pivot is found, positioned, then sublists recursively called

################################################################
################################################################
When it comes to the analysis of the Quick Sort Algorithm things can get confusing

This is because we can actually end up with quadratic time, Big-O(n**2)
Now this usually isn't the case, but these are those edge cases mentioned earlier
Where quadratic runtime is found, a combination or modification can greatly reduce it

We typically see a Big-O(n*log(n)) runtime complexity though
At the same time with precise analytics of the code and its constants
-> We can end up with smaller constants than merge sort on an average case

>We begin by picking a pivot element
-> This can be done in constant time; 'O(1)'

>Then we arrange all the elements relative to that pivot element
-> This step is possible in linear time; 'O(n)'

> Then we split the list into smaller sublists omitting the pivot element
-> This can be done in somewhat similar to logarithmic time; 'O(log(n))'

--> Therefore we end up with log(n) * linear operation; 'O(n*log(n))'

The problem arises with certain edge cases in which this runtime is false
Given list: [7, 5, 3, 2, 1]

We can see that the pivot element will always be list[-1]
This means we will move linearly across the length of the list
> initial = [7, 5, 3, 2, 1]
-> recur1 = [1, 7, 5, 3, 2]
-> recur2 = [1, 2, 7, 5, 3]
-> recur3 = [1, 2, 3, 7, 5]
-> sorted = [1, 2, 3, 5, 7]

Since were not actually splitting the list, just removing one element
This becomes quadratic time with n operations n times
This is really only the case if our pivot element is max/min element in every recursion
Anytime our pivot element is an edge case we only remove one element
-> So if were always only removing one element, were not doing any better than shit on a blanket

While this has roughly the same runtime complexity as Merge sort with O(n*log(n))
-> And typically we don't care about the constants, but here it makes a difference
If 2 algorithms like these have the same asymptotic growth
-> We can then begin to focus on constants, and reducing them, to determine the better application use algo

################################################################
################################################################
There are some hybrid sorting algorithms to mention that combine sorting algorithms
-> They can be considered more efficient in most cases than just a single sorting approach

1). Tim_Sort:
> This is the sorting algorithm actually used by Python when we use the sort() function
> It's a combination of quick sort and insertion sort
-> It begins by sorting with quick sort
--> It detects that it has to deal with an edge case, i.e.- every iter is a max/min pivot
--> It switches to insertion sort as insertion is more efficient in dealing with these edge cases
---> This is rare as insertion=O(quadratic) but we prefer that over quick's quadratic, rare cases though

################################################################
################################################################
Quick Sort Pseudocode:

    function quicksort(array)
        less, equal, greater := 3 empty arrays
        if array size > 1
            pivot := select any element of array
            for each x in array
                if x < pivot then add x to less
                if x = pivot then add x to equal
                if x > pivot then add x to greater
              quicksort(less)
              quicksort(greater)
              array := concatenate(less, equal, greater)


Another method(java-ish):

    function quicksort(array)
        if array size > 1
            choose a pivot
            while there are items left in array
                if item < pivot
                    put item into sublist1
                else
                    put item into sublist2
            quicksort(sublist1)
            quicksort(sublist2)












































